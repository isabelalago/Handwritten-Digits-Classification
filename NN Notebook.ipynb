{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension to reload modules before cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test the sigmoid function, and compare it to scipy's implementation to make sure we're getting the right return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import sigmoid # our sigmoid\n",
    "from scipy.special import expit # scipy's sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these should be the same\n",
    "print(sigmoid(1))\n",
    "print(expit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.linspace(-10,10,100):\n",
    "    assert(sigmoid(i) == expit(i)) # this shouldn't fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sanity check\n",
    "try:\n",
    "    assert(sigmoid(0.7) == expit(0.71)) # this should fail\n",
    "except AssertionError as e:\n",
    "    print(\"It Successfully Failed :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking vector values\n",
    "v = np.array([0.75, 0.3, -0.56, 0.01]) # random vector\n",
    "print(sigmoid(v))\n",
    "print(expit(v))\n",
    "print(\"Are they equal?\", sigmoid(v) == expit(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "\n",
    "     Some suggestions for preprocessing step:\n",
    "     - feature selection\"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    # Pick a reasonable size for validation data\n",
    "\n",
    "    # ------------Initialize preprocess arrays----------------------#\n",
    "    train_preprocess = np.zeros(shape=(50000, 784))\n",
    "    validation_preprocess = np.zeros(shape=(10000, 784))\n",
    "    test_preprocess = np.zeros(shape=(10000, 784))\n",
    "    train_label_preprocess = np.zeros(shape=(50000,))\n",
    "    validation_label_preprocess = np.zeros(shape=(10000,))\n",
    "    test_label_preprocess = np.zeros(shape=(10000,))\n",
    "    # ------------Initialize flag variables----------------------#\n",
    "    train_len = 0\n",
    "    validation_len = 0\n",
    "    test_len = 0\n",
    "    train_label_len = 0\n",
    "    validation_label_len = 0\n",
    "    # ------------Start to split the data set into 6 arrays-----------#\n",
    "    for key in mat:\n",
    "        # -----------when the set is training set--------------------#\n",
    "        if \"train\" in key:\n",
    "            label = key[-1]  # record the corresponding label\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)  # get the length of current training set\n",
    "            tag_len = tup_len - 1000  # defines the number of examples which will be added into the training set\n",
    "\n",
    "            # ---------------------adding data to training set-------------------------#\n",
    "            train_preprocess[train_len:train_len + tag_len] = tup[tup_perm[1000:], :]\n",
    "            train_len += tag_len\n",
    "\n",
    "            train_label_preprocess[train_label_len:train_label_len + tag_len] = label\n",
    "            train_label_len += tag_len\n",
    "\n",
    "            # ---------------------adding data to validation set-------------------------#\n",
    "            validation_preprocess[validation_len:validation_len + 1000] = tup[tup_perm[0:1000], :]\n",
    "            validation_len += 1000\n",
    "\n",
    "            validation_label_preprocess[validation_label_len:validation_label_len + 1000] = label\n",
    "            validation_label_len += 1000\n",
    "\n",
    "            # ---------------------adding data to test set-------------------------#\n",
    "        elif \"test\" in key:\n",
    "            label = key[-1]\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)\n",
    "            test_label_preprocess[test_len:test_len + tup_len] = label\n",
    "            test_preprocess[test_len:test_len + tup_len] = tup[tup_perm]\n",
    "            test_len += tup_len\n",
    "            # ---------------------Shuffle,double and normalize-------------------------#\n",
    "    train_size = range(train_preprocess.shape[0])\n",
    "    train_perm = np.random.permutation(train_size)\n",
    "    train_data = train_preprocess[train_perm]\n",
    "    train_data = np.double(train_data)\n",
    "    train_data = train_data / 255.0\n",
    "    train_label = train_label_preprocess[train_perm]\n",
    "\n",
    "    validation_size = range(validation_preprocess.shape[0])\n",
    "    vali_perm = np.random.permutation(validation_size)\n",
    "    validation_data = validation_preprocess[vali_perm]\n",
    "    validation_data = np.double(validation_data)\n",
    "    validation_data = validation_data / 255.0\n",
    "    validation_label = validation_label_preprocess[vali_perm]\n",
    "\n",
    "    test_size = range(test_preprocess.shape[0])\n",
    "    test_perm = np.random.permutation(test_size)\n",
    "    test_data = test_preprocess[test_perm]\n",
    "    test_data = np.double(test_data)\n",
    "    test_data = test_data / 255.0\n",
    "    test_label = test_label_preprocess[test_perm]\n",
    "\n",
    "    # Feature selection\n",
    "    # Your code here.\n",
    "\n",
    "    print('preprocess done')\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now loaded what would normally be the returned data of preprocess() to the above variables.  Now we want to figure out a way to check if a value is the same accross all rows for a given column.\n",
    "\n",
    "Since the algorthm will only be trained on 'train_data', we should only have to test this on that set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/14859458/how-to-check-if-all-values-in-the-columns-of-a-numpy-matrix-are-the-same\n",
    "(train_data == train_data[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(len(res)) # 784 == 28 x 28\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a vector of size 784 indicating `True` if the column is the same for all rows and `False` otherwise.  So, any column that is `True` here is giving us the same value accross the every training example.\n",
    "\n",
    "To make sure we're doing this correctly, we'll perform a similar check in a more intuitive, but disgusting inefficient way.  We see in the above example that the first entry is `True`, meaning that every row should share the same value (either `1` or `0`).\n",
    "\n",
    "So, we'll loop through every example and check if an entry is the same in every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "# 0th Entry is True\n",
    "t0 = train_data[0][0]\n",
    "for t in train_data:\n",
    "    if (t0 != t[0]):\n",
    "        print(\"Something's wrong\") # shouldn't print\n",
    "        \n",
    "# 100th Entry is True\n",
    "t0 = train_data[0][100]\n",
    "for t in train_data:\n",
    "    if (t0 != t[100]):\n",
    "        print(\"This one's right\") # should print\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To see the sum of these values\n",
    "for i in range(784):\n",
    "    s = 0\n",
    "    for t in train_data:\n",
    "        s += t[i]\n",
    "\n",
    "    print(i, res[i], s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the above function works to find these useless features.  We now only need to remove those features from the data sets and note the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "removable_indices = np.where(res)\n",
    "print(removable_indices) # indices of useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_data = np.delete(train_data, removable_indices, 1)\n",
    "print(train_data.shape)\n",
    "print(clean_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we removed columns from the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(clean_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_train_data = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(res_train_data)\n",
    "\n",
    "res_clean_data = np.all(clean_train_data == clean_train_data[0,:], axis = 0)\n",
    "print(res_clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(res_train_data))\n",
    "print(sum(res_clean_data)) #should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement this into the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(sum(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it seems to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnObjFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import preprocess, sigmoid, initializeWeights\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Features (array([ 12,  13,  14,  15,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  58,  59,  60,\n",
      "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
      "        74,  75,  76,  77,  78,  79,  80,  81,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 113, 114, 115, 116, 117, 118,\n",
      "       119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
      "       132, 133, 134, 135, 136, 137, 138, 139, 142, 143, 144, 145, 146,\n",
      "       147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,\n",
      "       160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173,\n",
      "       174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186,\n",
      "       187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199,\n",
      "       200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
      "       213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "       226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
      "       239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "       252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n",
      "       265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277,\n",
      "       278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290,\n",
      "       291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303,\n",
      "       304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
      "       317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
      "       330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
      "       343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
      "       356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 367, 368, 369,\n",
      "       370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382,\n",
      "       383, 384, 385, 386, 387, 388, 389, 390, 391, 393, 394, 395, 396,\n",
      "       397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409,\n",
      "       410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422,\n",
      "       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n",
      "       436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448,\n",
      "       449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "       462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474,\n",
      "       475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488,\n",
      "       489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501,\n",
      "       502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514,\n",
      "       515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527,\n",
      "       528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540,\n",
      "       541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553,\n",
      "       554, 555, 556, 557, 558, 559, 561, 562, 563, 564, 565, 566, 567,\n",
      "       568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580,\n",
      "       581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593,\n",
      "       594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606,\n",
      "       607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619,\n",
      "       620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632,\n",
      "       633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 646, 647,\n",
      "       648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660,\n",
      "       661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 674, 675, 676,\n",
      "       677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689,\n",
      "       690, 691, 692, 693, 694, 695, 696, 697, 698, 702, 703, 704, 705,\n",
      "       706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718,\n",
      "       719, 720, 721, 722, 723, 724, 725, 726, 731, 732, 733, 734, 735,\n",
      "       736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748,\n",
      "       749, 750, 751, 752, 753, 760, 761, 762, 763, 764, 765, 766, 767,\n",
      "       768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779]),)\n",
      "preprocess done\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "#  Train Neural Network\n",
    "\n",
    "# set the number of nodes in input unit (not including bias unit)\n",
    "n_input = train_data.shape[1]\n",
    "\n",
    "# set the number of nodes in hidden unit (not including bias unit)\n",
    "n_hidden = 50\n",
    "\n",
    "# set the number of nodes in output unit\n",
    "n_class = 10\n",
    "\n",
    "# initialize the weights into some random matrices\n",
    "initial_w1 = initializeWeights(n_input, n_hidden)\n",
    "initial_w2 = initializeWeights(n_hidden, n_class)\n",
    "\n",
    "# unroll 2 weight matrices into single column vector\n",
    "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)\n",
    "\n",
    "# set the regularization hyper-parameter\n",
    "lambdaval = 0\n",
    "\n",
    "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "# Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "\n",
    "opts = {'maxiter': 50}  # Preferred value.\n",
    "\n",
    "#nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnObjFunction(params, *args):\n",
    "    n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n",
    "    \n",
    "    training_data = training_data[:500]\n",
    "    training_label = training_label[:500]\n",
    "\n",
    "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "    obj_val = 0\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    # Set Bias\n",
    "    \n",
    "    b1 = np.ones((len(training_data), 1))\n",
    "    b2 = np.ones((len(training_data), 1))\n",
    "\n",
    "    # Forward Propagation\n",
    "    X = np.append(training_data, b1, 1) # append bias\n",
    "    net1 = X.dot(w1.T)\n",
    "    o1 = sigmoid(net1)\n",
    "    \n",
    "    H = np.append(o1, b2, 1)\n",
    "    net2 = H.dot(w2.T)\n",
    "    o2 = sigmoid(net2)\n",
    "    \n",
    "    # 1-hot encoding\n",
    "    y = np.zeros(o2.shape)\n",
    "    y[np.arange(o2.shape[0]), training_label.astype(int)] = 1\n",
    "    \n",
    "    # Error\n",
    "    E = (y*np.log(o2) + (np.ones(y.shape) - y)*np.log(np.ones(o2.shape) - o2))\n",
    "    obj_val = -(np.sum(E) / len(training_data))\n",
    "    \n",
    "    plt_data.append(obj_val)\n",
    "    \n",
    "    # Gradients\n",
    "    grad_w2 = np.dot((o2-y).T, H)\n",
    "    sm = (o2 - y).dot(w2[:,:-1]).T # note: we remove the bias from w2\n",
    "    tm = ((1-o1)*o1).T\n",
    "    grad_w1 = (sm * tm).dot(X)\n",
    "    \n",
    "    # Make sure you reshape the gradient matrices to a 1D array. for instance if your gradient matrices are grad_w1 and grad_w2\n",
    "    # you would use code similar to the one below to create a flat array\n",
    "    obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n",
    "    #obj_grad = np.array([])\n",
    "\n",
    "    return (obj_val, obj_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt_data = []\n",
    "nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.002830006918167662\n",
       "     jac: array([0.        , 0.        , 0.        , ..., 0.00921969, 0.0043071 ,\n",
       "       0.00283116])\n",
       " message: 'Maximum number of iterations has been exceeded.'\n",
       "    nfev: 272\n",
       "     nit: 50\n",
       "    njev: 272\n",
       "  status: 1\n",
       " success: False\n",
       "       x: array([-0.01831706, -0.05334407,  0.02017948, ..., -1.26102006,\n",
       "       -0.73427479, -0.37559822])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f640060c5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(plt_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot ought to show a pretty successful learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
